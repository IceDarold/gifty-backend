{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gifty Fast Scorer: Text-Based Giftability\n",
                "\n",
                "This worker performs an initial pass through the catalog to determine basic **giftability** using only text data (`content_text`).\n",
                "\n",
                "### Goal\n",
                "Quickly filter the catalog and assign a 0.0 - 1.0 score to prioritize items for deep 10-D vision scoring later.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip -q install -U transformers accelerate bitsandbytes requests tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import logging\n",
                "import torch\n",
                "import requests\n",
                "import time\n",
                "from tqdm.auto import tqdm\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "try:\n",
                "    from kaggle_secrets import UserSecretsClient\n",
                "    INTERNAL_TOKEN = UserSecretsClient().get_secret(\"INTERNAL_API_TOKEN\")\n",
                "except:\n",
                "    INTERNAL_TOKEN = os.getenv(\"INTERNAL_API_TOKEN\", \"default_token\")\n",
                "\n",
                "# --- Configuration ---\n",
                "API_BASE_URL = \"https://api.giftyai.ru\"\n",
                "MODEL_ID = \"Qwen/Qwen2-7B-Instruct\" # Using text-only Qwen for speed\n",
                "ENGINE_VERSION = \"v1.0-fast-text-only\"\n",
                "BATCH_LIMIT = 20\n",
                "\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
                "logger = logging.getLogger(\"FastScorer\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(f\"Loading {MODEL_ID}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID, \n",
                "    torch_dtype=\"auto\", \n",
                "    device_map=\"auto\"\n",
                ")\n",
                "model.eval()\n",
                "logger.info(\"Fast Text Model Loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SYSTEM_PROMPT = \"\"\"You are a professional gift curator.\n",
                "Analyze the product details and determine if this item is a GOOD GIFT.\n",
                "\n",
                "A good gift is:\n",
                "1. Emotionally valuable or highly practical for a specific person.\n",
                "2. Not a generic raw material (like raw wood) or industrial part.\n",
                "3. Suitable for giving (packaging/intent).\n",
                "\n",
                "OUTPUT FORMAT: Return ONLY a valid JSON object.\n",
                "JSON Structure:\n",
                "{\n",
                "  \"gift_score\": 0.0-1.0,  // 1.0 = Perfect gift, 0.0 = Not a gift at all\n",
                "  \"reasoning\": \"Brief explanation (1 sentence)\"\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "def extract_json(raw_text):\n",
                "    try:\n",
                "        content = raw_text[raw_text.find('{'):raw_text.rfind('}')+1]\n",
                "        return json.loads(content)\n",
                "    except:\n",
                "        return None\n",
                "\n",
                "def process_one(item):\n",
                "    context = item.get('content_text') or f\"Title: {item.get('title')}\\nDescription: {item.get('description')}\"\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "        {\"role\": \"user\", \"content\": f\"Evaluate this product:\\n{context}\"}\n",
                "    ]\n",
                "    \n",
                "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=150, do_sample=False)\n",
                "    \n",
                "    # Remove input ids from result\n",
                "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
                "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
                "    \n",
                "    return extract_json(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "headers = {\"X-Internal-Token\": INTERNAL_TOKEN}\n",
                "\n",
                "logger.info(\"Starting Fast Text Scoring Cycle...\")\n",
                "\n",
                "while True:\n",
                "    try:\n",
                "        # 1. Fetch tasks\n",
                "        repo_resp = requests.get(f\"{API_BASE_URL}/internal/scoring/tasks?limit={BATCH_LIMIT}\", headers=headers)\n",
                "        if repo_resp.status_code != 200:\n",
                "            logger.error(f\"API Error: {repo_resp.text}\")\n",
                "            time.sleep(30); continue\n",
                "            \n",
                "        tasks = repo_resp.json()\n",
                "        if not tasks:\n",
                "            logger.info(\"All items scored. Sleeping...\")\n",
                "            time.sleep(3600); continue\n",
                "            \n",
                "        logger.info(f\"Syncing {len(tasks)} items...\")\n",
                "        results = []\n",
                "        for t in tasks:\n",
                "            try:\n",
                "                data = process_one(t)\n",
                "                if data:\n",
                "                    results.append({\n",
                "                        \"gift_id\": t['gift_id'],\n",
                "                        \"llm_gift_score\": data.get('gift_score', 0.5),\n",
                "                        \"llm_gift_reasoning\": data.get('reasoning', ''),\n",
                "                        \"llm_scoring_model\": MODEL_ID,\n",
                "                        \"llm_scoring_version\": ENGINE_VERSION\n",
                "                    })\n",
                "            except Exception as e: logger.error(f\"Item {t['gift_id']} failed: {e}\")\n",
                "            \n",
                "        # 2. Submit\n",
                "        if results:\n",
                "            requests.post(f\"{API_BASE_URL}/internal/scoring/submit\", json={\"results\": results}, headers=headers)\n",
                "            logger.info(f\"Submitted {len(results)} items.\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        logger.error(f\"Global Error: {e}\")\n",
                "        time.sleep(60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}