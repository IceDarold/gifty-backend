{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f91cab84",
            "metadata": {},
            "source": [
                "# LLM 8B Scoring with Reasoning (CoT)\n",
                "\n",
                "This notebook implements product scoring using a local LLM (Qwen2.5-7B-Instruct). It first generates brief reasoning and then calculates a probability score for being a gift based on that reasoning."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6e636e6c",
            "metadata": {},
            "source": [
                "### Cell 1 — install"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce0ea0cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip -q install -U transformers accelerate bitsandbytes sentencepiece pandas tqdm pyarrow matplotlib seaborn"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "06fcdd43",
            "metadata": {},
            "source": [
                "### Cell 2 — load model (4-bit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be677971",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # 8B-ish\n",
                "\n",
                "bnb = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                ")\n",
                "\n",
                "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dc07fcad",
            "metadata": {},
            "source": [
                "### Cell 3 — scoring + reasoning logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aad7c87d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "SYSTEM = \"\"\"You are a strict giftability classifier.\n",
                "First, provide a very brief reasoning in Russian or English (max 2 sentences).\n",
                "Then conclude with 'Answer: GIFT' or 'Answer: NOT_GIFT'.\"\"\"\n",
                "\n",
                "def build_prompt(title, category=\"\", store=\"\", price_rub=None):\n",
                "    price_txt = \"\" if price_rub is None else f\"{price_rub}\"\n",
                "    user = f\"\"\"Decide if this product is a good gift item for most people.\n",
                "If it is mostly a utilitarian supply/chemical/spare part/consumable -> NOT_GIFT.\n",
                "If it is a presentable gift item (decor, gadgets, jewelry, toys, hobby items) -> GIFT.\n",
                "\n",
                "Product:\n",
                "- title: {title}\n",
                "- category: {category}\n",
                "- store: {store}\n",
                "- price_rub: {price_txt}\n",
                "Reasoning:\"\"\"\n",
                "    msgs = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM},\n",
                "        {\"role\": \"user\", \"content\": user},\n",
                "    ]\n",
                "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
                "\n",
                "@torch.no_grad()\n",
                "def score_label_after_reasoning(prompt_with_reasoning: str, label: str) -> float:\n",
                "    # Note: label should include the space if needed, e.g. \" GIFT\"\n",
                "    full = prompt_with_reasoning + label\n",
                "    enc_full = tok(full, return_tensors=\"pt\").to(model.device)\n",
                "    enc_prompt = tok(prompt_with_reasoning, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    input_ids = enc_full[\"input_ids\"]\n",
                "    prompt_len = enc_prompt[\"input_ids\"].shape[1]\n",
                "    logits = model(**enc_full).logits\n",
                "    \n",
                "    label_ids = input_ids[:, prompt_len:]\n",
                "    start = prompt_len - 1\n",
                "    lp = 0.0\n",
                "    for j in range(label_ids.shape[1]):\n",
                "        token_id = label_ids[0, j].item()\n",
                "        logp = F.log_softmax(logits[0, start + j, :], dim=-1)[token_id].item()\n",
                "        lp += logp\n",
                "    return lp\n",
                "\n",
                "@torch.no_grad()\n",
                "def giftability_with_reasoning(title, category=\"\", store=\"\", price_rub=None):\n",
                "    initial_prompt = build_prompt(title, category, store, price_rub)\n",
                "    \n",
                "    # 1. Generate reasoning until \"Answer:\"\n",
                "    inputs = tok(initial_prompt, return_tensors=\"pt\").to(model.device)\n",
                "    output_ids = model.generate(\n",
                "        **inputs, \n",
                "        max_new_tokens=60, \n",
                "        do_sample=False, \n",
                "        pad_token_id=tok.eos_token_id,\n",
                "        stopping_criteria=None # We'll just split by \"Answer:\" later\n",
                "    )\n",
                "    \n",
                "    full_text = tok.decode(output_ids[0], skip_special_tokens=True)\n",
                "    # Qwen suele repetir el prompt en generate dependendiendo de la config, \n",
                "    # pero skip_special_tokens + chat template suele devolver todo.\n",
                "    # Usamos tok.decode(output_ids[0][len(inputs.input_ids[0]):]) para solo lo nuevo\n",
                "    generated_only = tok.decode(output_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
                "    \n",
                "    # Split by \"Answer:\"\n",
                "    if \"Answer:\" in generated_only:\n",
                "        reasoning, _ = generated_only.split(\"Answer:\", 1)\n",
                "    else:\n",
                "        reasoning = generated_only\n",
                "    \n",
                "    reasoning = reasoning.strip()\n",
                "    \n",
                "    # 2. Score based on prompt + reasoning + \"Answer:\"\n",
                "    prompt_for_scoring = initial_prompt + reasoning + \"\\nAnswer:\"\n",
                "    \n",
                "    s_gift = score_label_after_reasoning(prompt_for_scoring, \" GIFT\")\n",
                "    s_not  = score_label_after_reasoning(prompt_for_scoring, \" NOT_GIFT\")\n",
                "    \n",
                "    p_gift = float(torch.softmax(torch.tensor([s_not, s_gift]), dim=0)[1].item())\n",
                "    \n",
                "    # Round to 2 decimals, < 0.01 -> 0\n",
                "    p_gift = round(p_gift, 2)\n",
                "    if p_gift < 0.01:\n",
                "        p_gift = 0.0\n",
                "        \n",
                "    return p_gift, reasoning"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "933083c8",
            "metadata": {},
            "source": [
                "### Cell 4 — Load Data (Parquet)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8af925e7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "\n",
                "PATH = \"/kaggle/input/gifty-takprodam/haiku_100_enriched.parquet\"\n",
                "df = pd.read_parquet(PATH)\n",
                "print(\"Rows loaded:\", len(df))\n",
                "df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ab10100-0cf7-40dd-b8ca-9592f9f3e8e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize and clean\n",
                "def to_float_safe(x):\n",
                "    if pd.isna(x): return np.nan\n",
                "    if isinstance(x, (int, float)): return float(x)\n",
                "    s = str(x).replace(\" \", \"\").replace(\",\", \".\")\n",
                "    s = re.sub(r\"[^0-9.]\", \"\", s)\n",
                "    try: return float(s) if s else np.nan\n",
                "    except: return np.nan\n",
                "\n",
                "if \"price\" not in df.columns and \"Цена товара\" in df.columns:\n",
                "    df[\"price\"] = df[\"Цена товара\"].apply(to_float_safe)\n",
                "\n",
                "df = df.dropna(subset=[\"title\", \"category\"]).reset_index(drop=True)\n",
                "print(\"Clean rows:\", len(df))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ad17ba67",
            "metadata": {},
            "source": [
                "### Cell 5 — scoring + reasoning (sample 50 - slow due to generation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0e5e9bb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "# Generation is slow, smaller sample by default\n",
                "df_sample = df.sample(min(50, len(df)), random_state=42).copy()\n",
                "\n",
                "scores = []\n",
                "reasonings = []\n",
                "for _, r in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
                "    p, reason = giftability_with_reasoning(\n",
                "        title=r.get(\"title\",\"\"),\n",
                "        category=r.get(\"category\",\"\"),\n",
                "        store=r.get(\"store_name\",\"\"),\n",
                "        price_rub=r.get(\"price\", None),\n",
                "    )\n",
                "    scores.append(p)\n",
                "    reasonings.append(reason)\n",
                "\n",
                "df_sample[\"llm_gift_score\"] = scores\n",
                "df_sample[\"llm_reasoning\"] = reasonings\n",
                "\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "df_sample[[\"title\", \"attr_is_giftable\", \"llm_gift_score\", \"llm_reasoning\"]].head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "analytics_md",
            "metadata": {},
            "source": [
                "### Cell 6 — Comparative Analytics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analytics_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# 1. Correlation\n",
                "corr = df_sample[[\"attr_is_giftable\", \"llm_gift_score\"]].corr().iloc[0, 1]\n",
                "print(f\"Correlation between Haiku baseline and Qwen2.5 (with reasoning): {corr:.4f}\")\n",
                "\n",
                "# 2. Distribution Plot\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.kdeplot(df_sample[\"attr_is_giftable\"], label=\"Haiku baseline\", fill=True)\n",
                "sns.kdeplot(df_sample[\"llm_gift_score\"], label=\"Qwen2.5 + Reasoning\", fill=True)\n",
                "plt.title(\"Giftability Score Distribution Comparison\")\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# 3. Top disagreements\n",
                "df_sample[\"score_diff\"] = df_sample[\"llm_gift_score\"] - df_sample[\"attr_is_giftable\"]\n",
                "print(\"\\n--- Top 5 Disagreements (LLM > Haiku) ---\")\n",
                "display(df_sample.sort_values(\"score_diff\", ascending=False)[[\"title\", \"attr_is_giftable\", \"llm_gift_score\", \"llm_reasoning\"]].head(5))\n",
                "\n",
                "print(\"\\n--- Top 5 Disagreements (Haiku > LLM) ---\")\n",
                "display(df_sample.sort_values(\"score_diff\", ascending=True)[[\"title\", \"attr_is_giftable\", \"llm_gift_score\", \"llm_reasoning\"]].head(5))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}