{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f91cab84",
            "metadata": {},
            "source": [
                "# LLM 8B Scoring vs Haiku baseline\n",
                "\n",
                "This notebook implements product scoring using a local LLM (Qwen2.5-7B-Instruct) and compares results with the `attr_is_giftable` baseline from Haiku."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6e636e6c",
            "metadata": {},
            "source": [
                "### Cell 1 — install"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce0ea0cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip -q install -U transformers accelerate bitsandbytes sentencepiece pandas tqdm pyarrow matplotlib seaborn"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "06fcdd43",
            "metadata": {},
            "source": [
                "### Cell 2 — load model (4-bit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be677971",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # 8B-ish\n",
                "\n",
                "bnb = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                ")\n",
                "\n",
                "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dc07fcad",
            "metadata": {},
            "source": [
                "### Cell 3 — scoring logic (logprobs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aad7c87d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "SYSTEM = \"You are a strict classifier. Answer with ONLY one token: GIFT or NOT_GIFT.\"\n",
                "\n",
                "def build_prompt(title, category=\"\", store=\"\", price_rub=None):\n",
                "    price_txt = \"\" if price_rub is None else f\"{price_rub}\"\n",
                "    user = f\"\"\"Decide if this product is a good gift item for most people.\n",
                "If it is mostly a utilitarian supply/chemical/spare part/consumable -> NOT_GIFT.\n",
                "If it is a presentable gift item (decor, gadgets, jewelry, toys, hobby items) -> GIFT.\n",
                "\n",
                "Product:\n",
                "- title: {title}\n",
                "- category: {category}\n",
                "- store: {store}\n",
                "- price_rub: {price_txt}\n",
                "Answer:\"\"\"\n",
                "    msgs = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM},\n",
                "        {\"role\": \"user\", \"content\": user},\n",
                "    ]\n",
                "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
                "\n",
                "@torch.no_grad()\n",
                "def score_label(prompt: str, label: str) -> float:\n",
                "    full = prompt + label\n",
                "    enc_full = tok(full, return_tensors=\"pt\").to(model.device)\n",
                "    enc_prompt = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    input_ids = enc_full[\"input_ids\"]\n",
                "    prompt_len = enc_prompt[\"input_ids\"].shape[1]\n",
                "    logits = model(**enc_full).logits\n",
                "    label_ids = input_ids[:, prompt_len:]\n",
                "    start = prompt_len - 1\n",
                "    lp = 0.0\n",
                "    for j in range(label_ids.shape[1]):\n",
                "        token_id = label_ids[0, j].item()\n",
                "        logp = F.log_softmax(logits[0, start + j, :], dim=-1)[token_id].item()\n",
                "        lp += logp\n",
                "    return lp\n",
                "\n",
                "@torch.no_grad()\n",
                "def giftability_llm(title, category=\"\", store=\"\", price_rub=None):\n",
                "    prompt = build_prompt(title, category, store, price_rub)\n",
                "    s_gift = score_label(prompt, \" GIFT\")\n",
                "    s_not  = score_label(prompt, \" NOT_GIFT\")\n",
                "    p_gift = float(torch.softmax(torch.tensor([s_not, s_gift]), dim=0)[1].item())\n",
                "    return p_gift"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "933083c8",
            "metadata": {},
            "source": [
                "### Cell 4 — Load Data (Parquet)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8af925e7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "\n",
                "PATH = \"/kaggle/input/gifty-takprodam/haiku_100_enriched.parquet\"\n",
                "df = pd.read_parquet(PATH)\n",
                "print(\"Rows loaded:\", len(df))\n",
                "df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ab10100-0cf7-40dd-b8ca-9592f9f3e8e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize and clean\n",
                "def to_float_safe(x):\n",
                "    if pd.isna(x): return np.nan\n",
                "    if isinstance(x, (int, float)): return float(x)\n",
                "    s = str(x).replace(\" \", \"\").replace(\",\", \".\")\n",
                "    s = re.sub(r\"[^0-9.]\", \"\", s)\n",
                "    try: return float(s) if s else np.nan\n",
                "    except: return np.nan\n",
                "\n",
                "if \"price\" not in df.columns and \"Цена товара\" in df.columns:\n",
                "    df[\"price\"] = df[\"Цена товара\"].apply(to_float_safe)\n",
                "\n",
                "df = df.dropna(subset=[\"title\", \"category\"]).reset_index(drop=True)\n",
                "print(\"Clean rows:\", len(df))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ad17ba67",
            "metadata": {},
            "source": [
                "### Cell 5 — scoring (sample 200)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0e5e9bb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "df_sample = df.sample(min(200, len(df)), random_state=42).copy()\n",
                "\n",
                "scores = []\n",
                "for _, r in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
                "    p = giftability_llm(\n",
                "        title=r.get(\"title\",\"\"),\n",
                "        category=r.get(\"category\",\"\"),\n",
                "        store=r.get(\"store_name\",\"\"),\n",
                "        price_rub=r.get(\"price\", None),\n",
                "    )\n",
                "    scores.append(p)\n",
                "\n",
                "df_sample[\"llm_gift_score\"] = scores\n",
                "df_sample[[\"title\", \"category\", \"attr_is_giftable\", \"llm_gift_score\"]].head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "analytics_md",
            "metadata": {},
            "source": [
                "### Cell 6 — Comparative Analytics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analytics_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# 1. Correlation\n",
                "corr = df_sample[[\"attr_is_giftable\", \"llm_gift_score\"]].corr().iloc[0, 1]\n",
                "print(f\"Correlation between Haiku baseline and Qwen2.5: {corr:.4f}\")\n",
                "\n",
                "# 2. Distribution Plot\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.kdeplot(df_sample[\"attr_is_giftable\"], label=\"Haiku baseline\", fill=True)\n",
                "sns.kdeplot(df_sample[\"llm_gift_score\"], label=\"Qwen2.5 (local)\", fill=True)\n",
                "plt.title(\"Giftability Score Distribution Comparison\")\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# 3. Disagreement analysis (Difference)\n",
                "df_sample[\"score_diff\"] = df_sample[\"llm_gift_score\"] - df_sample[\"attr_is_giftable\"]\n",
                "df_sample[\"abs_diff\"] = df_sample[\"score_diff\"].abs()\n",
                "\n",
                "print(\"\\n--- Top 10 Disagreements (LLM thinks GIFT, Haiku thinks NOT) ---\")\n",
                "display(df_sample.sort_values(\"score_diff\", ascending=False)[[\"title\", \"category\", \"attr_is_giftable\", \"llm_gift_score\"]].head(10))\n",
                "\n",
                "print(\"\\n--- Top 10 Disagreements (Haiku thinks GIFT, LLM thinks NOT) ---\")\n",
                "display(df_sample.sort_values(\"score_diff\", ascending=True)[[\"title\", \"category\", \"attr_is_giftable\", \"llm_gift_score\"]].head(10))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}