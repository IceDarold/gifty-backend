{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Gifty Production Worker: LLM Scoring\n",
                "\n",
                "This notebook connects to the Gifty Internal API, fetches products that need scoring, and pushes the results back to the database. Optimized for 2x T4 GPUs.\n",
                "\n",
                "### Setup Guide\n",
                "1. Set `API_BASE_URL` to your backend URL (e.g. `https://api.gifty.gift`).\n",
                "2. Set `INTERNAL_TOKEN` to match the value in your backend settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "install",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip -q install -U transformers accelerate bitsandbytes sentencepiece pandas tqdm requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import sys\n",
                "\n",
                "API_BASE_URL = \"https://your-api-url.com\" # Update this\n",
                "INTERNAL_TOKEN = \"default_internal_token\"  # Update this\n",
                "DEBUG = True # Set to True for verbose logging\n",
                "\n",
                "MODEL_ID = \"Qwen/Qwen2.5-32B-Instruct\"\n",
                "MODEL_VERSION = \"v1.1\" # Prompt version\n",
                "MODEL_TAG = \"qwen2.5-32b-4bit\" # Hardware/quantization tag\n",
                "\n",
                "# Configure Logging\n",
                "log_level = logging.DEBUG if DEBUG else logging.INFO\n",
                "logging.basicConfig(\n",
                "    level=log_level,\n",
                "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
                "    handlers=[\n",
                "        logging.StreamHandler(sys.stdout)\n",
                "    ]\n",
                ")\n",
                "logger = logging.getLogger(\"GiftyWorker\")\n",
                "logger.info(f\"Logger initialized with level: {logging.getLevelName(log_level)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "logger.info(f\"Loading model {MODEL_ID}...\")\n",
                "bnb = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                ")\n",
                "\n",
                "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "model.eval()\n",
                "logger.info(\"Model loaded successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "logic",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "SYSTEM = \"\"\"You are a strict giftability classifier.\n",
                "First, provide a very brief reasoning in Russian or English (max 2 sentences).\n",
                "Then conclude with 'Answer: GIFT' or 'Answer: NOT_GIFT'.\"\"\"\n",
                "\n",
                "def build_prompt(title, category=\"\", merchant=\"\", price=None):\n",
                "    user = f\"\"\"Decide if this product is a good gift item for most people.\n",
                "Utilitarian/chemical/spare parts -> NOT_GIFT. \n",
                "Decor/gadgets/jewelry/toys -> GIFT.\n",
                "\n",
                "Product:\n",
                "- title: {title}\n",
                "- category: {category}\n",
                "- merchant: {merchant}\n",
                "- price: {price}\n",
                "Reasoning:\"\"\"\n",
                "    msgs = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM},\n",
                "        {\"role\": \"user\", \"content\": user},\n",
                "    ]\n",
                "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
                "\n",
                "@torch.no_grad()\n",
                "def score_label(prompt_with_reasoning: str, label: str) -> float:\n",
                "    full = prompt_with_reasoning + label\n",
                "    enc_full = tok(full, return_tensors=\"pt\").to(model.device)\n",
                "    enc_prompt = tok(prompt_with_reasoning, return_tensors=\"pt\").to(model.device)\n",
                "    logits = model(**enc_full).logits\n",
                "    prompt_len = enc_prompt[\"input_ids\"].shape[1]\n",
                "    label_ids = enc_full[\"input_ids\"][:, prompt_len:]\n",
                "    lp = 0.0\n",
                "    for j in range(label_ids.shape[1]):\n",
                "        token_id = label_ids[0, j].item()\n",
                "        logp = F.log_softmax(logits[0, prompt_len - 1 + j, :], dim=-1)[token_id].item()\n",
                "        lp += logp\n",
                "    return lp\n",
                "\n",
                "def process_one(item):\n",
                "    logger.debug(f\"Processing item: {item.get('title', 'N/A')} (ID: {item.get('gift_id')})\")\n",
                "    \n",
                "    prompt = build_prompt(item.get('title',''), item.get('category',''), item.get('merchant',''), item.get('price'))\n",
                "    if DEBUG: # Show full prompt only in debug mode\n",
                "        logger.debug(f\"--- PROMPT ---\\n{prompt}\\n--------------\")\n",
                "    \n",
                "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    out = model.generate(**inputs, max_new_tokens=80, do_sample=False, pad_token_id=tok.eos_token_id)\n",
                "    gen = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
                "    \n",
                "    logger.debug(f\"Raw generation: {gen}\")\n",
                "    \n",
                "    reasoning = gen.split(\"Answer:\")[0].strip() if \"Answer:\" in gen else gen.strip()\n",
                "    score_prompt = prompt + reasoning + \"\\nAnswer:\"\n",
                "    \n",
                "    s_gift = score_label(score_prompt, \" GIFT\")\n",
                "    s_not = score_label(score_prompt, \" NOT_GIFT\")\n",
                "    \n",
                "    logger.debug(f\"Logprobs -> GIFT: {s_gift:.4f}, NOT_GIFT: {s_not:.4f}\")\n",
                "    \n",
                "    p = float(torch.softmax(torch.tensor([s_not, s_gift]), dim=0)[1].item())\n",
                "    \n",
                "    p_final = round(p, 2)\n",
                "    if p_final < 0.01: p_final = 0.0\n",
                "    \n",
                "    logger.info(f\"Result: {p_final} | Reason: {reasoning}\")\n",
                "    return p_final, reasoning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import time\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "headers = {\"X-Internal-Token\": INTERNAL_TOKEN}\n",
                "\n",
                "logger.info(\"!!! Starting production worker loop !!!\")\n",
                "while True:\n",
                "    try:\n",
                "        # 1. Get tasks\n",
                "        logger.debug(f\"Fetching tasks from {API_BASE_URL}...\")\n",
                "        resp = requests.get(f\"{API_BASE_URL}/internal/scoring/tasks?limit=20\", headers=headers, timeout=30)\n",
                "        if resp.status_code != 200:\n",
                "            logger.error(f\"Error fetching tasks (HTTP {resp.status_code}): {resp.text}\")\n",
                "            time.sleep(30)\n",
                "            continue\n",
                "            \n",
                "        tasks = resp.json()\n",
                "        if not tasks:\n",
                "            logger.info(\"No more products to score. Waiting 5 minutes...\")\n",
                "            time.sleep(300)\n",
                "            continue\n",
                "            \n",
                "        logger.info(f\"[Batch] Processing {len(tasks)} items\")\n",
                "        results = []\n",
                "        for t in tasks:\n",
                "            try:\n",
                "                p, reason = process_one(t)\n",
                "                results.append({\n",
                "                    \"gift_id\": t['gift_id'],\n",
                "                    \"llm_gift_score\": p,\n",
                "                    \"llm_gift_reasoning\": reason,\n",
                "                    \"llm_scoring_model\": MODEL_TAG,\n",
                "                    \"llm_scoring_version\": MODEL_VERSION\n",
                "                })\n",
                "            except Exception as e_one:\n",
                "                logger.error(f\"Error processing item {t.get('gift_id')}: {e_one}\", exc_info=True)\n",
                "            \n",
                "        # 2. Submit results\n",
                "        if results:\n",
                "            logger.debug(f\"Submitting {len(results)} results back to API...\")\n",
                "            s_resp = requests.post(f\"{API_BASE_URL}/internal/scoring/submit\", json={\"results\": results}, headers=headers, timeout=30)\n",
                "            if s_resp.status_code == 200:\n",
                "                logger.info(f\"Successfully updated {s_resp.json().get('updated')} items.\")\n",
                "            else:\n",
                "                logger.error(f\"Failed to submit results (HTTP {s_resp.status_code}): {s_resp.text}\")\n",
                "        else:\n",
                "            logger.warning(\"Batch finished with 0 results to submit.\")\n",
                "            \n",
                "    except requests.exceptions.RequestException as re:\n",
                "        logger.error(f\"Network error: {re}\")\n",
                "        time.sleep(30)\n",
                "    except Exception as e:\n",
                "        logger.error(f\"Unexpected error in main loop: {e}\", exc_info=True)\n",
                "        time.sleep(30)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}