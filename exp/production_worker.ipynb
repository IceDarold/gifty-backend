{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Gifty Production Worker: LLM Scoring\n",
                "\n",
                "This notebook connects to the Gifty Internal API, fetches products that need scoring, and pushes the results back to the database. Optimized for 2x T4 GPUs.\n",
                "\n",
                "### Setup Guide\n",
                "1. Set `API_BASE_URL` to your backend URL (e.g. `https://api.gifty.gift`).\n",
                "2. Set `INTERNAL_TOKEN` to match the value in your backend settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "install",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip -q install -U transformers accelerate bitsandbytes sentencepiece pandas tqdm requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "API_BASE_URL = \"https://your-api-url.com\" # Update this\n",
                "INTERNAL_TOKEN = \"default_internal_token\"  # Update this\n",
                "\n",
                "MODEL_ID = \"Qwen/Qwen2.5-32B-Instruct\"\n",
                "MODEL_VERSION = \"v1.0\" # Prompt version\n",
                "MODEL_TAG = \"qwen2.5-32b-4bit\" # Hardware/quantization tag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "bnb = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                ")\n",
                "\n",
                "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "logic",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "SYSTEM = \"\"\"You are a strict giftability classifier.\n",
                "First, provide a very brief reasoning in Russian or English (max 2 sentences).\n",
                "Then conclude with 'Answer: GIFT' or 'Answer: NOT_GIFT'.\"\"\"\n",
                "\n",
                "def build_prompt(title, category=\"\", merchant=\"\", price=None):\n",
                "    user = f\"\"\"Decide if this product is a good gift item for most people.\n",
                "Utilitarian/chemical/spare parts -> NOT_GIFT. \n",
                "Decor/gadgets/jewelry/toys -> GIFT.\n",
                "\n",
                "Product:\n",
                "- title: {title}\n",
                "- category: {category}\n",
                "- merchant: {merchant}\n",
                "- price: {price}\n",
                "Reasoning:\"\"\"\n",
                "    msgs = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM},\n",
                "        {\"role\": \"user\", \"content\": user},\n",
                "    ]\n",
                "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
                "\n",
                "@torch.no_grad()\n",
                "def score_label(prompt_with_reasoning: str, label: str) -> float:\n",
                "    full = prompt_with_reasoning + label\n",
                "    enc_full = tok(full, return_tensors=\"pt\").to(model.device)\n",
                "    enc_prompt = tok(prompt_with_reasoning, return_tensors=\"pt\").to(model.device)\n",
                "    logits = model(**enc_full).logits\n",
                "    prompt_len = enc_prompt[\"input_ids\"].shape[1]\n",
                "    label_ids = enc_full[\"input_ids\"][:, prompt_len:]\n",
                "    lp = 0.0\n",
                "    for j in range(label_ids.shape[1]):\n",
                "        token_id = label_ids[0, j].item()\n",
                "        logp = F.log_softmax(logits[0, prompt_len - 1 + j, :], dim=-1)[token_id].item()\n",
                "        lp += logp\n",
                "    return lp\n",
                "\n",
                "def process_one(item):\n",
                "    prompt = build_prompt(item['title'], item['category'], item['merchant'], item['price'])\n",
                "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    out = model.generate(**inputs, max_new_tokens=80, do_sample=False, pad_token_id=tok.eos_token_id)\n",
                "    gen = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
                "    \n",
                "    reasoning = gen.split(\"Answer:\")[0].strip() if \"Answer:\" in gen else gen.strip()\n",
                "    score_prompt = prompt + reasoning + \"\\nAnswer:\"\n",
                "    \n",
                "    s_gift = score_label(score_prompt, \" GIFT\")\n",
                "    s_not = score_label(score_prompt, \" NOT_GIFT\")\n",
                "    p = float(torch.softmax(torch.tensor([s_not, s_gift]), dim=0)[1].item())\n",
                "    \n",
                "    p = round(p, 2)\n",
                "    if p < 0.01: p = 0.0\n",
                "    return p, reasoning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "\n",
                "headers = {\"X-Internal-Token\": INTERNAL_TOKEN}\n",
                "\n",
                "print(\"Starting worker loop...\")\n",
                "while True:\n",
                "    try:\n",
                "        # 1. Get tasks\n",
                "        resp = requests.get(f\"{API_BASE_URL}/internal/scoring/tasks?limit=20\", headers=headers)\n",
                "        if resp.status_code != 200:\n",
                "            print(f\"Error fetching tasks: {resp.status_code}\")\n",
                "            time.sleep(30)\n",
                "            continue\n",
                "            \n",
                "        tasks = resp.json()\n",
                "        if not tasks:\n",
                "            print(\"No more products to score. Waiting 5 minutes...\")\n",
                "            time.sleep(300)\n",
                "            continue\n",
                "            \n",
                "        print(f\"Processing batch of {len(tasks)} items...\")\n",
                "        results = []\n",
                "        for t in tqdm(tasks):\n",
                "            p, reason = process_one(t)\n",
                "            results.append({\n",
                "                \"gift_id\": t['gift_id'],\n",
                "                \"llm_gift_score\": p,\n",
                "                \"llm_gift_reasoning\": reason,\n",
                "                \"llm_scoring_model\": MODEL_TAG,\n",
                "                \"llm_scoring_version\": MODEL_VERSION\n",
                "            })\n",
                "            \n",
                "        # 2. Submit results\n",
                "        s_resp = requests.post(f\"{API_BASE_URL}/internal/scoring/submit\", json={\"results\": results}, headers=headers)\n",
                "        if s_resp.status_code == 200:\n",
                "            print(f\"Successfully updated {s_resp.json().get('updated')} items.\")\n",
                "        else:\n",
                "            print(f\"Failed to submit results: {s_resp.status_code}\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Unexpected error: {e}\")\n",
                "        time.sleep(30)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}