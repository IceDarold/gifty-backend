{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Gifty Vision Worker: Multimodal LLM Scoring\n",
    "\n",
    "This worker uses **Qwen2-VL-7B-Instruct** to evaluate product giftability using both text and images. \n",
    "\n",
    "### Capabilities:\n",
    "1. **Visual Analysis**: Sees the product's design, quality, and packaging.\n",
    "2. **Text Analysis**: Understands title, category, and merchant.\n",
    "3. **Logprob Scoring**: Provides a precise probability score based on multimodal context.\n",
    "\n",
    "### Requirements:\n",
    "1. Setup your `INTERNAL_API_TOKEN` in Kaggle Secrets (Add-ons -> Secrets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers accelerate bitsandbytes qwen_vl_utils requests Pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "API_BASE_URL = \"https://api.giftyai.ru\"\n",
    "DEBUG = True\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    INTERNAL_TOKEN = user_secrets.get_secret(\"INTERNAL_API_TOKEN\")\n",
    "except Exception:\n",
    "    print(\"Warning: Could not fetch INTERNAL_API_TOKEN from Kaggle Secrets. Falling back to environment.\")\n",
    "    INTERNAL_TOKEN = os.getenv(\"INTERNAL_API_TOKEN\", \"default_internal_token\")\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "MODEL_VERSION = \"v2.0-vision\"\n",
    "MODEL_TAG = \"qwen2-vl-7b-fp16\"\n",
    "\n",
    "# Configure Logging\n",
    "log_level = logging.DEBUG if DEBUG else logging.INFO\n",
    "logging.basicConfig(\n",
    "    level=log_level,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(\"GiftyVisionWorker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "logger.info(f\"Loading Vision Model {MODEL_ID}...\")\n",
    "\n",
    "# On 2x T4 (32GB), we can load 7B in full float16 for maximum vision precision\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model.eval()\n",
    "logger.info(\"Multimodal model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def get_image(url):\n",
    "    try:\n",
    "        if not url: return None\n",
    "        response = requests.get(url, timeout=10)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to load image from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_vision_prompt(item):\n",
    "    # We construct messages for the processor\n",
    "    content = []\n",
    "    if item.get('image_url'):\n",
    "        content.append({\"type\": \"image\", \"image\": item['image_url']})\n",
    "    \n",
    "    text_query = f\"\"\"Decide if this product is a good gift item for most people.\n",
    "Analyze the image and text. \n",
    "Utilitarian/chemical/spare parts -> NOT_GIFT. \n",
    "Decor/gadgets/jewelry/toys -> GIFT.\n",
    "\n",
    "Product:\n",
    "- title: {item.get('title')}\n",
    "- category: {item.get('category')}\n",
    "- merchant: {item.get('merchant')}\n",
    "\n",
    "Briefly explain your reasoning (2 sentences max), then conclude with Answer: GIFT or Answer: NOT_GIFT.\"\"\"\n",
    "    \n",
    "    content.append({\"type\": \"text\", \"text\": text_query})\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "    return messages\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_label_multimodal(full_prompt_text, image_inputs, label: str):\n",
    "    # Note: Scoring VLM with logprobs is similar to text scoring\n",
    "    # We need to compute logits for the suffix label\n",
    "    full_text = full_prompt_text + label\n",
    "    \n",
    "    # Process again with the full text + images\n",
    "    inputs = processor(\n",
    "        text=[full_text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Prompt only inputs to find where label starts\n",
    "    prompt_inputs = processor(\n",
    "        text=[full_prompt_text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    prompt_len = prompt_inputs.input_ids.shape[1]\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    label_ids = inputs.input_ids[:, prompt_len:]\n",
    "    lp = 0.0\n",
    "    for j in range(label_ids.shape[1]):\n",
    "        token_id = label_ids[0, j].item()\n",
    "        logp = F.log_softmax(logits[0, prompt_len - 1 + j, :], dim=-1)[token_id].item()\n",
    "        lp += logp\n",
    "    return lp\n",
    "\n",
    "def process_one_vision(item):\n",
    "    logger.debug(f\"Processing VLM item: {item.get('title')} (ID: {item.get('gift_id')})\")\n",
    "    \n",
    "    messages = build_vision_prompt(item)\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 1. Generate Reasoning\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    gen_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    reasoning = gen_text.split(\"Answer:\")[0].strip() if \"Answer:\" in gen_text else gen_text.strip()\n",
    "    logger.debug(f\"VLM Reasoning: {reasoning}\")\n",
    "    \n",
    "    # 2. Score Probabilities\n",
    "    scoring_prompt = text + reasoning + \"\\nAnswer:\"\n",
    "    \n",
    "    s_gift = score_label_multimodal(scoring_prompt, image_inputs, \" GIFT\")\n",
    "    s_not = score_label_multimodal(scoring_prompt, image_inputs, \" NOT_GIFT\")\n",
    "    \n",
    "    p = float(torch.softmax(torch.tensor([s_not, s_gift]), dim=0)[1].item())\n",
    "    p_final = round(p, 2)\n",
    "    if p_final < 0.01: p_final = 0.0\n",
    "    \n",
    "    logger.info(f\"Result: {p_final} | {item.get('title')[:30]}...\")\n",
    "    return p_final, reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "headers = {\"X-Internal-Token\": INTERNAL_TOKEN}\n",
    "\n",
    "logger.info(\"!!! VISION WORKER STARTING (api.giftyai.ru) !!!\")\n",
    "while True:\n",
    "    try:\n",
    "        # 1. Get tasks\n",
    "        resp = requests.get(f\"{API_BASE_URL}/internal/scoring/tasks?limit=10\", headers=headers, timeout=30)\n",
    "        if resp.status_code != 200:\n",
    "            logger.error(f\"Error fetching tasks: {resp.status_code} | {resp.text}\")\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "            \n",
    "        tasks = resp.json()\n",
    "        if not tasks:\n",
    "            logger.info(\"No tasks found. Sleeping 5 min.\")\n",
    "            time.sleep(300)\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"[Batch] Processing {len(tasks)} items with vision\")\n",
    "        results = []\n",
    "        for t in tasks:\n",
    "            try:\n",
    "                p, reason = process_one_vision(t)\n",
    "                results.append({\n",
    "                    \"gift_id\": t['gift_id'],\n",
    "                    \"llm_gift_score\": p,\n",
    "                    \"llm_gift_reasoning\": reason,\n",
    "                    \"llm_scoring_model\": MODEL_TAG,\n",
    "                    \"llm_scoring_version\": MODEL_VERSION\n",
    "                })\n",
    "            except Exception as e_item:\n",
    "                logger.error(f\"Item {t.get('gift_id')} failed: {e_item}\")\n",
    "                \n",
    "        # 2. Submit results\n",
    "        if results:\n",
    "            s_resp = requests.post(f\"{API_BASE_URL}/internal/scoring/submit\", json={\"results\": results}, headers=headers, timeout=30)\n",
    "            if s_resp.status_code == 200:\n",
    "                logger.info(f\"Successfully updated {s_resp.json().get('updated')} items.\")\n",
    "            else:\n",
    "                logger.error(f\"Submission failed: {s_resp.status_code}\")\n",
    "                \n",
    "    except Exception as e_main:\n",
    "        logger.error(f\"Main loop error: {e_main}\", exc_info=True)\n",
    "        time.sleep(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}