{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéÅ Gifty AI Worker: Product Scorer\n",
                "\n",
                "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ **Kaggle** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **GPU (T4 x2)**.\n",
                "–û–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–µ—Ä–µ—Ç —Ç–æ–≤–∞—Ä—ã –∏–∑ —Å–∏—Å—Ç–µ–º—ã Gifty, –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö ¬´–ø–æ–¥–∞—Ä–æ—á–Ω–æ—Å—Ç—å¬ª —Å –ø–æ–º–æ—â—å—é LLM –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ.\n",
                "\n",
                "### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\n",
                "1. –í–∫–ª—é—á–∏—Ç–µ **Accelerator: T4 x2** –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö (Settings -> Accelerator).\n",
                "2. –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ –≤ —Å–µ–∫—Ü–∏—é **Configuration** (API_BASE –∏ API_TOKEN).\n",
                "3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch accelerate requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import time\n",
                "import json\n",
                "import torch\n",
                "import logging\n",
                "from transformers import pipeline\n",
                "from kaggle_secrets import UserSecretsClient\n",
                "\n",
                "class GiftyInternalClient:\n",
                "    def __init__(self, api_base, token):\n",
                "        self.api_base = api_base.rstrip(\"/\")\n",
                "        self.headers = {\"X-Internal-Token\": token}\n",
                "        self.logger = logging.getLogger(\"GiftyWorker\")\n",
                "\n",
                "    def get_scoring_tasks(self, limit=50):\n",
                "        url = f\"{self.api_base}/internal/scoring/tasks?limit={limit}\"\n",
                "        resp = requests.get(url, headers=self.headers)\n",
                "        resp.raise_for_status()\n",
                "        return resp.json()\n",
                "\n",
                "    def submit_scoring(self, results):\n",
                "        url = f\"{self.api_base}/internal/scoring/submit\"\n",
                "        resp = requests.post(url, json={\"results\": results}, headers=self.headers)\n",
                "        resp.raise_for_status()\n",
                "        return resp.json()\n",
                "\n",
                "# –ü–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –ò–ò\n",
                "def extract_json(text):\n",
                "    try:\n",
                "        start = text.find('{')\n",
                "        end = text.rfind('}') + 1\n",
                "        return json.loads(text[start:end])\n",
                "    except: \n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Configuration\n",
                "–ó–∞–ø–æ–ª–Ω–∏ —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ –Ω–∏–∂–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π Kaggle Secrets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
                "API_BASE = \"https://api.giftyai.ru\" # <-- –ó–ê–ú–ï–ù–ò –ù–ê –°–í–û–ô\n",
                "\n",
                "try:\n",
                "    user_secrets = UserSecretsClient()\n",
                "    API_TOKEN = user_secrets.get_secret(\"INTERNAL_API_TOKEN\")\n",
                "except:\n",
                "    API_TOKEN = \"your_secret_token_here\" # <-- –ò–õ–ò –í–ü–ò–®–ò –¢–£–¢\n",
                "\n",
                "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏\n",
                "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
                "BATCH_SIZE = 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü§ñ Model Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Loading model {MODEL_NAME}...\")\n",
                "pipe = pipeline(\n",
                "    \"text-generation\", \n",
                "    model=MODEL_NAME, \n",
                "    device_map=\"auto\", \n",
                "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèÉ‚Äç‚ôÇÔ∏è Worker Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_one_item(task):\n",
                "    prompt = f\"\"\"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–æ–≤–∞—Ä –∏ –æ—Ü–µ–Ω–∏ –µ–≥–æ –∫–∞–∫ –ø–æ–¥–∞—Ä–æ–∫.\n",
                "    –ù–∞–∑–≤–∞–Ω–∏–µ: {task['title']}\n",
                "    –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {task.get('category')}\n",
                "    –û–ø–∏—Å–∞–Ω–∏–µ: {task.get('content_text')}\n",
                "    \n",
                "    –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON —Ñ–æ—Ä–º–∞—Ç:\n",
                "    {{\n",
                "        \"score\": 0.0-10.0, \n",
                "        \"reasoning\": \"–∫—Ä–∞—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\"\n",
                "    }}\"\"\"\n",
                "    \n",
                "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
                "    response = pipe(messages, max_new_tokens=256, return_full_text=False)[0]['generated_text']\n",
                "    \n",
                "    data = extract_json(response)\n",
                "    if not data:\n",
                "        return 5.0, \"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞\" # fallback\n",
                "        \n",
                "    return data.get('score', 5.0), data.get('reasoning', \"–ë–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\")\n",
                "\n",
                "client = GiftyInternalClient(API_BASE, API_TOKEN)\n",
                "print(\"Worker started...\")\n",
                "\n",
                "while True:\n",
                "    try:\n",
                "        tasks = client.get_scoring_tasks(limit=BATCH_SIZE)\n",
                "        if not tasks:\n",
                "            print(\"No tasks found. Waiting 60s...\")\n",
                "            time.sleep(60)\n",
                "            continue\n",
                "            \n",
                "        print(f\"Processing batch of {len(tasks)} items...\")\n",
                "        batch_results = []\n",
                "        \n",
                "        for task in tasks:\n",
                "            score, reason = process_one_item(task)\n",
                "            batch_results.append({\n",
                "                \"gift_id\": task['gift_id'],\n",
                "                \"llm_gift_score\": score,\n",
                "                \"llm_gift_reasoning\": reason,\n",
                "                \"llm_scoring_model\": MODEL_NAME,\n",
                "                \"llm_scoring_version\": \"v1.0\"\n",
                "            })\n",
                "            \n",
                "        client.submit_scoring(batch_results)\n",
                "        print(f\"Successfully submitted results for {len(tasks)} items.\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error in loop: {e}\")\n",
                "        time.sleep(10)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}