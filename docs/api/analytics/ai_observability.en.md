# AI Observability ðŸ§ 

Real-time tracking system for AI model performance. Allows monitoring costs, response quality, and latency.

### AI Usage Statistics (`aiUsage`)

Aggregated data for all LLM requests over a specified period.

**Query:**
```graphql
query {
  aiUsage(days: 7) {
    totalTokens
    totalCostUsd
    avgLatencyMs
    requestsCount
    providerDistribution
    lastUpdated
  }
}
```

**Field Descriptions:**
- `totalTokens`: Total amount of tokens (input + output).
- `totalCostUsd`: Estimated cost of requests in USD.
- `avgLatencyMs`: Average model response time.
- `providerDistribution`: Distribution of requests by provider (e.g., {"anthropic": 150, "gemini": 50}).

---

### LLM Call Logs (`llmLogs`)

Detailed list of recent interactions with language models.

**Query:**
```graphql
query {
  llmLogs(limit: 10) {
    id
    provider
    model
    callType
    latencyMs
    totalTokens
    costUsd
    createdAt
  }
}
```

**Call Types (`callType`):**
- `normalize_topics`: Standardization of user interests.
- `classify_topic`: Determining topic depth.
- `generate_hypotheses`: Gift idea generation.
- `probe_exploration`: Clarifying questions.

---

### Quality Analysis (`hypothesisDetails`)

You can analyze a specific hypothesis to see what search queries were generated by AI and what products were found.

**Query:**
```graphql
query {
  hypothesisDetails(id: "HYPOTHESIS-UUID") {
    title
    track
    reaction
    searchQueries
    totalResultsFound
    products {
      giftId
      score
      rank
      clicked
    }
  }
}
```
